{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbdc8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 14:35:16.662870: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 14:35:17.077538: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64\n",
      "2023-01-11 14:35:17.077651: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-11 14:35:19.060507: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64\n",
      "2023-01-11 14:35:19.060847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64\n",
      "2023-01-11 14:35:19.060881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fb3d6",
   "metadata": {},
   "source": [
    "### 이미지 높이 400으로 거정, 비율이 맞게끔 너비 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c69a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array, save_img\n",
    "\n",
    "# 변환하려는 이미지 경로\n",
    "target_image_path = './protrait.jpg'\n",
    "\n",
    "# 스타일 이미지 경로\n",
    "style_reference_image_path = './popova.jpg'\n",
    "\n",
    "#생성된 사진의 차원\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)    # 비율에 맞춰서 줄여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91180f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d8fc121",
   "metadata": {},
   "source": [
    "### VGG19 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9479ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from keras.applications import vgg19\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # VGG19.preprocessing_input 함수에서 일어나는 변환을 복원하기 위해 ImageNet의 평균 픽셀 값을 더하고\n",
    "    \n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # 'BGR' -> 'RGB' 로 변환\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493a453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabf18d5",
   "metadata": {},
   "source": [
    "### VGG19 네트워크 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794205f",
   "metadata": {},
   "source": [
    "### input_tensor는 타깃이미지, 스타일이미지, 생성된 이미지를 행으로 쌓게됨\n",
    "### 결국 input_tensor의 차원은 (3, 400, width, 3)이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "314507e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# from keras import backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 이미 준비되어 있는 이미지이므로  constant를 이용해서 정의\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "\n",
    "# 생성된 이미지를 담을 플레이스홀더\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "# 세 개의 이미지를 하나의 배치로 합침\n",
    "input_tensor = K.concatenate([target_image,\n",
    "                              style_reference_image,\n",
    "                              combination_image], axis=0)\n",
    "\n",
    "# 세 이미지의 배치를 입력으로 받는 VGG 네트워크를 만듦\n",
    "# 이 모델은 사전 훈련된 ImageNet 가중치를 로드\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)\n",
    "print('모델 로드 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c390251",
   "metadata": {},
   "source": [
    "### 콘텐츠 손실과 스타일 손실을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3537e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894db256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0f7631",
   "metadata": {},
   "source": [
    "### 두 손실 함수에 하나의 손실함수를 더 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "940cea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(x[:, :img_height-1, :img_width-1, :] - x[:, 1:, :img_width-1, :])\n",
    "    b = K.square(x[:, :img_height-1, :img_width-1, :] - x[:, :img_height-1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f826785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8257cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras2",
   "language": "python",
   "name": "keras2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
