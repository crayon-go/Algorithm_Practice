{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801461b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 16:59:42.408490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import typing\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae0acef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b78255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"LayerScale as introduced in CaiT: https://arxiv.org/abs/2103.17239. - Going deeper with Image Transformers\n",
    "\n",
    "    Args:\n",
    "        init_values (float): LayerScale의 diagonal matrix 초기값.\n",
    "        projection_dim (int): LayerScale에서 사용되는 projection dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values: float, projection_dim: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #tf.ones((projection_dim,)) -> [projection_dim,1] shape의 1로 이루어진 vector [1,1,...,1]\n",
    "        self.gamma = tf.Variable(init_values * tf.ones((projection_dim,)))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        return x * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAttention(layers.Layer):\n",
    "    \"\"\"Class attention as proposed in CaiT: https://arxiv.org/abs/2103.17239. - Going deeper with Image Transformers\n",
    "\n",
    "    Args:\n",
    "        projection_dim (int): attention에서 사용되는 query, key, value의 projection dimension \n",
    "        num_heads      (int): attention heads의 갯수.\n",
    "        dropout_rate (float): attention scores와 final projected outputs에서 사용될 dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, projection_dim: int, num_heads: int, dropout_rate: float, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        head_dim = projection_dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.q = layers.Dense(projection_dim)\n",
    "        self.k = layers.Dense(projection_dim)\n",
    "        self.v = layers.Dense(projection_dim)\n",
    "        self.attn_drop = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(projection_dim)\n",
    "        self.proj_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        batch_size, num_patches, num_channels = (\n",
    "            tf.shape(x)[0],\n",
    "            tf.shape(x)[1],\n",
    "            tf.shape(x)[2],\n",
    "        )\n",
    "\n",
    "        # Query projection. `cls_token` embeddings이 queries로 사용됩니다.\n",
    "        q = tf.expand_dims(self.q(x[:, 0]), axis=1)\n",
    "        q = tf.reshape(q, (batch_size, 1, self.num_heads, num_channels // self.num_heads))  \n",
    "       \n",
    "        # Shape: (batch_size, 1, num_heads, dimension_per_head)\n",
    "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
    "        scale = tf.cast(self.scale, dtype=q.dtype)\n",
    "        q = q * scale\n",
    "\n",
    "        # Key projection. Patch embeddings과 cls embedding이 keys로 사용됩니다.\n",
    "        k = self.k(x)\n",
    "        k = tf.reshape(k, (batch_size, num_patches, self.num_heads, num_channels // self.num_heads))  \n",
    "        \n",
    "        # Shape: (batch_size, num_tokens, num_heads, dimension_per_head)\n",
    "        k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Value projection. Patch embeddings과 cls embedding이 values로 사용됩니다.\n",
    "        v = self.v(x)\n",
    "        v = tf.reshape(v, (batch_size, num_patches, self.num_heads, num_channels // self.num_heads))\n",
    "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
    "\n",
    "        #cls_token embedding과 patch embeddings의 attention scores 계산하는 부분입니다.\n",
    "        attn = tf.matmul(q, k, transpose_b=True)\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn, training)\n",
    "\n",
    "        x_cls = tf.matmul(attn, v)\n",
    "        x_cls = tf.transpose(x_cls, perm=[0, 2, 1, 3])\n",
    "        x_cls = tf.reshape(x_cls, (batch_size, 1, num_channels))\n",
    "        x_cls = self.proj(x_cls)\n",
    "        x_cls = self.proj_drop(x_cls, training)\n",
    "\n",
    "        return x_cls, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e66573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c16c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eff598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841c98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199e735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800239d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b1ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf0196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16bf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df290a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a7a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ad646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646b931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b2098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackerrank",
   "language": "python",
   "name": "hackerrank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
